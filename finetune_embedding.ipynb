{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "28a8b793",
      "metadata": {
        "id": "28a8b793"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/finetuning/embeddings/finetune_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551753b7-6cd2-4f81-aec0-da119e4705ad",
      "metadata": {
        "id": "551753b7-6cd2-4f81-aec0-da119e4705ad"
      },
      "source": [
        "# Finetune Embeddings\n",
        "\n",
        "In this notebook, we show users how to finetune their own embedding models.\n",
        "\n",
        "We go through three main sections:\n",
        "1. Preparing the data (our `generate_qa_embedding_pairs` function makes this easy)\n",
        "2. Finetuning the model (using our `SentenceTransformersFinetuneEngine`)\n",
        "3. Evaluating the model on a validation knowledge corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99afd542-fc47-44ac-aed0-b3684108dba5",
      "metadata": {
        "id": "99afd542-fc47-44ac-aed0-b3684108dba5"
      },
      "source": [
        "## Generate Corpus\n",
        "\n",
        "First, we create the corpus of text chunks by leveraging LlamaIndex to load some financial PDFs, and parsing/chunking into plain text chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e973679e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e973679e",
        "outputId": "efb3658f-ccc0-40f9-8b2f-42cd4150a15e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index-llms-openai in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (0.2.11)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.7 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-llms-openai) (0.11.16)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-llms-openai) (1.47.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (4.6.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.11.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (3.22.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.7->llama-index-llms-openai) (24.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (0.2.5)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-embeddings-openai) (0.11.16)\n",
            "Requirement already satisfied: openai>=1.1.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-embeddings-openai) (1.47.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.6.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.11.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (3.22.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-openai) (24.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: llama-index-finetuning in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (0.2.1)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-finetuning) (0.11.16)\n",
            "Requirement already satisfied: llama-index-embeddings-adapter<0.3.0,>=0.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-finetuning) (0.2.2)\n",
            "Requirement already satisfied: llama-index-llms-azure-openai<0.3.0,>=0.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-finetuning) (0.2.1)\n",
            "Requirement already satisfied: llama-index-llms-mistralai<0.3.0,>=0.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-finetuning) (0.2.5)\n",
            "Requirement already satisfied: llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-finetuning) (0.2.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-finetuning) (3.1.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.16.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-embeddings-adapter<0.3.0,>=0.2.0->llama-index-finetuning) (2.2.2)\n",
            "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (1.18.0)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (0.2.11)\n",
            "Requirement already satisfied: mistralai>=1.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-llms-mistralai<0.3.0,>=0.2.0->llama-index-finetuning) (1.1.0)\n",
            "Requirement already satisfied: cohere<6.0.0,>=5.1.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (5.11.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sentence-transformers>=2.3.0->llama-index-finetuning) (4.44.2)\n",
            "Requirement already satisfied: scikit-learn in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sentence-transformers>=2.3.0->llama-index-finetuning) (1.5.2)\n",
            "Requirement already satisfied: scipy in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sentence-transformers>=2.3.0->llama-index-finetuning) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sentence-transformers>=2.3.0->llama-index-finetuning) (0.25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.11.1)\n",
            "Requirement already satisfied: azure-core>=1.31.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (1.31.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (43.0.1)\n",
            "Requirement already satisfied: msal>=1.30.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (1.31.0)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (1.2.0)\n",
            "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.35.34)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.9.7)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.4.0)\n",
            "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.9.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.23.4)\n",
            "Requirement already satisfied: sagemaker<3.0.0,>=2.232.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.232.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.19.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.32.0.20240914)\n",
            "Requirement already satisfied: anyio in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (4.6.0)\n",
            "Requirement already satisfied: certifi in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.0.5)\n",
            "Requirement already satisfied: idna in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (3.10)\n",
            "Requirement already satisfied: sniffio in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (0.14.0)\n",
            "Requirement already satisfied: filelock in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->sentence-transformers>=2.3.0->llama-index-finetuning) (3.16.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->sentence-transformers>=2.3.0->llama-index-finetuning) (24.1)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-llms-openai<0.3.0,>=0.2.1->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (1.47.0)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mistralai>=1.0.0->llama-index-llms-mistralai<0.3.0,>=0.2.0->llama-index-finetuning) (0.2.0)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mistralai>=1.0.0->llama-index-llms-mistralai<0.3.0,>=0.2.0->llama-index-finetuning) (1.0.6)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mistralai>=1.0.0->llama-index-llms-mistralai<0.3.0,>=0.2.0->llama-index-finetuning) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from python-dateutil==2.8.2->mistralai>=1.0.0->llama-index-llms-mistralai<0.3.0,>=0.2.0->llama-index-finetuning) (1.16.0)\n",
            "Requirement already satisfied: click in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (3.1.1)\n",
            "Requirement already satisfied: sympy in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from torch>=2.0.0->llama-index-embeddings-adapter<0.3.0,>=0.2.0->llama-index-finetuning) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from torch>=2.0.0->llama-index-embeddings-adapter<0.3.0,>=0.2.0->llama-index-finetuning) (3.1.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers>=2.3.0->llama-index-finetuning) (0.4.5)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-finetuning) (3.22.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.3.0->llama-index-finetuning) (3.5.0)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.34 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.35.34)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.10.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (1.17.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (2.9.0)\n",
            "Requirement already satisfied: portalocker<3,>=1.4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (2.10.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.1->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.1->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (0.5.0)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.2.1)\n",
            "Requirement already satisfied: docker in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (7.1.0)\n",
            "Requirement already satisfied: google-pasta in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.2.0)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (6.11.0)\n",
            "Requirement already satisfied: jsonschema in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (4.23.0)\n",
            "Requirement already satisfied: pandas in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.2.3)\n",
            "Requirement already satisfied: pathos in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.3.3)\n",
            "Requirement already satisfied: platformdirs in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (4.3.6)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.12 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (4.25.5)\n",
            "Requirement already satisfied: psutil in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (6.0.0)\n",
            "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.0.10)\n",
            "Requirement already satisfied: sagemaker-mlflow in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.1.0)\n",
            "Requirement already satisfied: schema in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.7.7)\n",
            "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.0.1)\n",
            "Requirement already satisfied: tblib<4,>=1.7.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->llama-index-embeddings-adapter<0.3.0,>=0.2.0->llama-index-finetuning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sympy->torch>=2.0.0->llama-index-embeddings-adapter<0.3.0,>=0.2.0->llama-index-finetuning) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.3.0,>=0.2.0->llama-index-finetuning) (2.22)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.20.2)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (13.9.2)\n",
            "Requirement already satisfied: mock<5.0,>4.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (4.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.20.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pandas->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pandas->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2024.1)\n",
            "Requirement already satisfied: ppft>=1.7.6.9 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.7.6.9)\n",
            "Requirement already satisfied: dill>=0.3.9 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.3.9)\n",
            "Requirement already satisfied: pox>=0.3.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.3.5)\n",
            "Requirement already satisfied: multiprocess>=0.70.17 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.70.17)\n",
            "Requirement already satisfied: mlflow>=2.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.16.2)\n",
            "Requirement already satisfied: mlflow-skinny==2.16.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.16.2)\n",
            "Requirement already satisfied: Flask<4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.0.3)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.13.3)\n",
            "Requirement already satisfied: graphene<4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.3)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.9.2)\n",
            "Requirement already satisfied: pyarrow<18,>=4.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (17.0.0)\n",
            "Requirement already satisfied: gunicorn<24 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (23.0.0)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (5.5.0)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.33.0)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.1.43)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.27.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.5.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.18.0)\n",
            "Requirement already satisfied: Mako in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.3.5)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.0.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.8.2)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.2.4)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.2.0)\n",
            "Requirement already satisfied: aniso8601<10,>=8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (9.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (3.1.4)\n",
            "Requirement already satisfied: google-auth~=2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (2.35.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (4.0.11)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.48b0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (5.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank<0.3.0,>=0.2.0->llama-index-finetuning) (0.6.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting llama-index-readers-file\n",
            "  Downloading llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file)\n",
            "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-readers-file) (0.11.16)\n",
            "Requirement already satisfied: pandas in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-readers-file) (2.2.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file)\n",
            "  Using cached pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file)\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pandas->llama-index-readers-file) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pandas->llama-index-readers-file) (2024.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.11.1)\n",
            "Requirement already satisfied: click in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.22.0)\n",
            "Requirement already satisfied: anyio in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (4.6.0)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/ledger/anaconda3/envs/policy-rag/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (24.1)\n",
            "Downloading llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
            "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "Using cached pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: striprtf, soupsieve, pypdf, beautifulsoup4, llama-index-readers-file\n",
            "Successfully installed beautifulsoup4-4.12.3 llama-index-readers-file-0.2.2 pypdf-4.3.1 soupsieve-2.6 striprtf-0.0.26\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-llms-openai\n",
        "%pip install llama-index-embeddings-openai\n",
        "%pip install llama-index-finetuning\n",
        "%pip install llama-index-readers-file\n",
        "%pip install optimum[exporters]\n",
        "%pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9280d438-b6bd-4ccf-a730-7c8bb3ebdbeb",
      "metadata": {
        "id": "9280d438-b6bd-4ccf-a730-7c8bb3ebdbeb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.schema import MetadataMode"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c42620",
      "metadata": {
        "id": "73c42620"
      },
      "source": [
        "Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e11b0c",
      "metadata": {
        "id": "d8e11b0c"
      },
      "outputs": [],
      "source": [
        "!mkdir -p 'data/10k/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c5e890bc-557b-4d3c-bede-3e80dfeeee18",
      "metadata": {
        "id": "c5e890bc-557b-4d3c-bede-3e80dfeeee18"
      },
      "outputs": [],
      "source": [
        "TRAIN_FILES = 'data'\n",
        "\n",
        "TRAIN_CORPUS_FPATH = \"train_corpus.json\"\n",
        "VAL_CORPUS_FPATH = \"val_corpus.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1da871c1-9d58-467a-92fd-06ed3d94534b",
      "metadata": {
        "id": "1da871c1-9d58-467a-92fd-06ed3d94534b"
      },
      "outputs": [],
      "source": [
        "def load_corpus(dir, verbose=False):\n",
        "    if verbose:\n",
        "        print(f\"Loading files {dir}\")\n",
        "\n",
        "    reader = SimpleDirectoryReader(dir)\n",
        "    docs = reader.load_data()\n",
        "    if verbose:\n",
        "        print(f\"Loaded {len(docs)} docs\")\n",
        "\n",
        "    parser = SentenceSplitter()\n",
        "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Parsed {len(nodes)} nodes\")\n",
        "\n",
        "    return nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53056d8b-3b4c-4364-9b07-a375aa84330b",
      "metadata": {
        "id": "53056d8b-3b4c-4364-9b07-a375aa84330b"
      },
      "source": [
        "We do a very naive train/val split by having the Lyft corpus as the train dataset, and the Uber corpus as the val dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d3651c77-d085-4fbc-bb34-61f143ad6674",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "554a6636780246c8a19d1efe7a6e4786",
            "6748733283a34725ba6365f3c1fb1c1d"
          ]
        },
        "id": "d3651c77-d085-4fbc-bb34-61f143ad6674",
        "outputId": "68e4fa24-3280-46e7-9694-762d6de05628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading files data\n",
            "Loaded 307 docs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parsing nodes: 100%|| 307/307 [00:00<00:00, 990.90it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsed 325 nodes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "corpus_nodes = load_corpus(TRAIN_FILES, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3456ebde",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val Nodes: 32\n",
            "Train Size: 293\n",
            "Val Size: 32\n"
          ]
        }
      ],
      "source": [
        "n_val_nodes = int(len(corpus_nodes) * 0.1)\n",
        "print(f'Val Nodes: {n_val_nodes}')\n",
        "\n",
        "train_nodes = corpus_nodes[:-n_val_nodes]\n",
        "print(f'Train Size: {len(train_nodes)}')\n",
        "\n",
        "val_nodes = corpus_nodes[len(train_nodes):]\n",
        "print(f'Val Size: {len(val_nodes)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "87619dee",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4482c48-844b-448b-9552-3f38b455645c",
      "metadata": {
        "id": "b4482c48-844b-448b-9552-3f38b455645c"
      },
      "source": [
        "### Generate synthetic queries\n",
        "\n",
        "Now, we use an LLM (gpt-3.5-turbo) to generate questions using each text chunk in the corpus as context.\n",
        "\n",
        "Each pair of (generated question, text chunk used as context) becomes a datapoint in the finetuning dataset (either for training or evaluation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "580334ce-ddaa-4cc0-8c3e-7294d11e4d2f",
      "metadata": {
        "id": "580334ce-ddaa-4cc0-8c3e-7294d11e4d2f"
      },
      "outputs": [],
      "source": [
        "from llama_index.finetuning import generate_qa_embedding_pairs\n",
        "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "666001e2",
      "metadata": {
        "id": "666001e2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "ef43fe59-a29c-481b-b086-e98e55016d3e",
      "metadata": {
        "id": "ef43fe59-a29c-481b-b086-e98e55016d3e",
        "outputId": "ebcf6b8b-c827-4d2c-ae26-3a1519107d02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/293 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/293 [00:00<03:56,  1.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 2/293 [00:01<04:32,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 3/293 [00:02<04:13,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|         | 4/293 [00:03<04:20,  1.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 5/293 [00:05<06:05,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 6/293 [00:06<05:32,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|         | 7/293 [00:07<05:24,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 8/293 [00:08<05:08,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 9/293 [00:09<04:48,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 10/293 [00:10<04:52,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|         | 11/293 [00:11<04:39,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|         | 12/293 [00:12<04:28,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|         | 13/293 [00:13<04:26,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|         | 14/293 [00:14<04:37,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|         | 15/293 [00:17<08:01,  1.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|         | 16/293 [00:18<06:36,  1.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|         | 17/293 [00:20<07:09,  1.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|         | 18/293 [00:21<06:06,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|         | 19/293 [00:21<05:24,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|         | 20/293 [00:22<04:55,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|         | 21/293 [00:24<06:01,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|         | 22/293 [00:26<06:00,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|         | 23/293 [00:27<06:03,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|         | 24/293 [00:28<06:17,  1.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|         | 25/293 [00:30<06:01,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|         | 26/293 [00:32<07:09,  1.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|         | 27/293 [00:33<06:45,  1.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|         | 28/293 [00:34<05:58,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|         | 29/293 [00:35<05:12,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|         | 30/293 [00:36<04:59,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|         | 31/293 [00:37<04:58,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|         | 32/293 [00:38<04:38,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|        | 33/293 [00:39<04:33,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|        | 34/293 [00:40<04:49,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|        | 35/293 [00:41<04:38,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|        | 36/293 [00:42<04:30,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|        | 37/293 [00:44<04:45,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|        | 38/293 [00:45<04:55,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|        | 39/293 [00:47<05:51,  1.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|        | 40/293 [00:48<05:38,  1.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|        | 41/293 [00:49<05:07,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|        | 42/293 [00:50<05:20,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|        | 43/293 [00:51<04:49,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|        | 44/293 [00:52<04:24,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|        | 45/293 [00:53<04:15,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|        | 46/293 [00:54<04:12,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|        | 47/293 [00:55<03:42,  1.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|        | 48/293 [00:56<03:48,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|        | 49/293 [00:56<03:36,  1.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|        | 50/293 [00:57<03:33,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|        | 51/293 [00:58<03:50,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|        | 52/293 [00:59<03:47,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|        | 53/293 [01:00<03:59,  1.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|        | 54/293 [01:01<04:03,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|        | 55/293 [01:03<04:29,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|        | 56/293 [01:04<04:21,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|        | 57/293 [01:05<04:20,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|        | 58/293 [01:06<04:11,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|        | 59/293 [01:07<04:39,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|        | 60/293 [01:09<04:43,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|        | 61/293 [01:10<04:45,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|        | 62/293 [01:11<04:09,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|       | 63/293 [01:12<04:10,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|       | 64/293 [01:13<04:12,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|       | 65/293 [01:14<03:58,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|       | 66/293 [01:15<03:51,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|       | 67/293 [01:16<03:44,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|       | 68/293 [01:17<03:35,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|       | 69/293 [01:18<04:20,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|       | 70/293 [01:19<04:07,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|       | 71/293 [01:20<04:09,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|       | 72/293 [01:22<04:16,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|       | 73/293 [01:23<04:25,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|       | 74/293 [01:24<04:06,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|       | 75/293 [01:25<04:11,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|       | 76/293 [01:26<03:59,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|       | 77/293 [01:27<03:55,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|       | 78/293 [01:28<03:35,  1.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|       | 79/293 [01:29<03:43,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|       | 80/293 [01:30<03:20,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|       | 81/293 [01:31<03:14,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|       | 82/293 [01:32<03:23,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|       | 83/293 [01:33<03:30,  1.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|       | 84/293 [01:34<03:15,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|       | 85/293 [01:35<03:22,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|       | 86/293 [01:36<03:35,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|       | 87/293 [01:37<03:24,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|       | 88/293 [01:38<03:10,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|       | 89/293 [01:39<03:15,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|       | 90/293 [01:40<03:22,  1.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|       | 91/293 [01:41<03:23,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|      | 92/293 [01:42<03:32,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|      | 93/293 [01:43<03:20,  1.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|      | 94/293 [01:44<03:26,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|      | 95/293 [01:45<03:23,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|      | 96/293 [01:46<03:21,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|      | 97/293 [01:47<03:22,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|      | 98/293 [01:48<03:19,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|      | 99/293 [01:49<03:27,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|      | 100/293 [01:50<03:14,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|      | 101/293 [01:51<03:36,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|      | 102/293 [01:53<03:38,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|      | 103/293 [01:53<03:19,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|      | 104/293 [01:55<03:22,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|      | 105/293 [01:55<03:12,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|      | 106/293 [01:56<03:05,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|      | 107/293 [01:57<02:55,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|      | 108/293 [01:58<02:53,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|      | 109/293 [02:01<04:31,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|      | 110/293 [02:02<04:06,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|      | 111/293 [02:03<03:37,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|      | 112/293 [02:04<03:28,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|      | 113/293 [02:05<03:12,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|      | 114/293 [02:06<03:14,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|      | 115/293 [02:07<02:59,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|      | 116/293 [02:08<02:59,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|      | 117/293 [02:09<03:25,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|      | 118/293 [02:11<03:42,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|      | 119/293 [02:12<03:29,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|      | 120/293 [02:13<03:18,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|     | 121/293 [02:14<03:06,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|     | 122/293 [02:15<03:33,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|     | 123/293 [02:16<03:13,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|     | 124/293 [02:18<03:39,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|     | 125/293 [02:19<03:19,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|     | 126/293 [02:20<03:03,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|     | 127/293 [02:21<02:58,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|     | 128/293 [02:22<02:46,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|     | 129/293 [02:23<02:48,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|     | 130/293 [02:24<02:45,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|     | 131/293 [02:25<03:04,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|     | 132/293 [02:26<02:58,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|     | 133/293 [02:27<02:57,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|     | 134/293 [02:28<02:47,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|     | 135/293 [02:29<02:44,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|     | 136/293 [02:30<02:35,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|     | 137/293 [02:31<02:49,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|     | 138/293 [02:33<03:03,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|     | 139/293 [02:34<02:44,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|     | 140/293 [02:34<02:32,  1.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|     | 141/293 [02:35<02:24,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|     | 142/293 [02:36<02:16,  1.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|     | 143/293 [02:37<02:15,  1.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|     | 144/293 [02:38<02:16,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|     | 145/293 [02:39<02:15,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|     | 146/293 [02:40<02:16,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|     | 147/293 [02:41<02:22,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|     | 148/293 [02:42<02:32,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|     | 149/293 [02:43<02:23,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|     | 150/293 [02:44<02:21,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|    | 151/293 [02:45<02:25,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|    | 152/293 [02:46<02:29,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|    | 153/293 [02:48<02:42,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|    | 154/293 [02:49<02:37,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|    | 155/293 [02:50<02:37,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|    | 156/293 [02:51<02:39,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|    | 157/293 [02:52<02:25,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|    | 158/293 [02:53<02:20,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|    | 159/293 [02:54<02:18,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|    | 160/293 [02:55<02:27,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|    | 161/293 [02:56<02:18,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|    | 162/293 [02:57<02:19,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|    | 163/293 [02:59<02:32,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|    | 164/293 [03:00<02:37,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|    | 165/293 [03:01<02:26,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|    | 166/293 [03:02<02:30,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|    | 167/293 [03:03<02:34,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|    | 168/293 [03:04<02:22,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|    | 169/293 [03:06<02:20,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|    | 170/293 [03:06<02:11,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|    | 171/293 [03:08<02:38,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|    | 172/293 [03:09<02:27,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|    | 173/293 [03:11<02:31,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|    | 174/293 [03:12<02:26,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|    | 175/293 [03:13<02:16,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|    | 176/293 [03:14<02:09,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|    | 177/293 [03:15<02:06,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|    | 178/293 [03:16<02:09,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|    | 179/293 [03:17<02:18,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|   | 180/293 [03:18<02:05,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|   | 181/293 [03:19<02:04,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|   | 182/293 [03:20<01:51,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|   | 183/293 [03:21<01:52,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|   | 184/293 [03:22<01:48,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|   | 185/293 [03:23<01:44,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|   | 186/293 [03:24<01:52,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|   | 187/293 [03:26<02:23,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|   | 188/293 [03:27<02:12,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|   | 189/293 [03:28<02:02,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|   | 190/293 [03:30<02:13,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|   | 191/293 [03:31<02:15,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|   | 192/293 [03:33<02:10,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|   | 193/293 [03:33<01:56,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|   | 194/293 [03:34<01:45,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|   | 195/293 [03:35<01:39,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|   | 196/293 [03:36<01:37,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|   | 197/293 [03:37<01:35,  1.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|   | 198/293 [03:39<01:44,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|   | 199/293 [03:40<01:56,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|   | 200/293 [03:41<01:43,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|   | 201/293 [03:42<01:40,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|   | 202/293 [03:43<01:36,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|   | 203/293 [03:45<01:59,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|   | 204/293 [03:46<01:52,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|   | 205/293 [03:47<01:46,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|   | 206/293 [03:49<01:56,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|   | 207/293 [03:50<01:44,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|   | 208/293 [03:51<01:35,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|  | 209/293 [03:52<01:29,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|  | 210/293 [03:53<01:37,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|  | 211/293 [03:54<01:27,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|  | 212/293 [03:55<01:22,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|  | 213/293 [03:55<01:14,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|  | 214/293 [03:56<01:15,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|  | 215/293 [03:57<01:17,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|  | 216/293 [03:59<01:18,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|  | 217/293 [04:00<01:22,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|  | 218/293 [04:01<01:21,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|  | 219/293 [04:02<01:15,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|  | 220/293 [04:03<01:12,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|  | 221/293 [04:03<01:05,  1.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|  | 222/293 [04:05<01:11,  1.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|  | 223/293 [04:05<01:04,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|  | 224/293 [04:07<01:11,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|  | 225/293 [04:08<01:10,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|  | 226/293 [04:09<01:09,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|  | 227/293 [04:10<01:05,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|  | 228/293 [04:11<01:04,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|  | 229/293 [04:12<01:03,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|  | 230/293 [04:12<00:58,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|  | 231/293 [04:13<00:58,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|  | 232/293 [04:15<01:04,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|  | 233/293 [04:16<01:09,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|  | 234/293 [04:18<01:19,  1.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|  | 235/293 [04:19<01:15,  1.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|  | 236/293 [04:20<01:11,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|  | 237/293 [04:21<01:06,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|  | 238/293 [04:23<01:07,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%| | 239/293 [04:23<01:00,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%| | 240/293 [04:24<00:57,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%| | 241/293 [04:26<00:59,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%| | 242/293 [04:27<00:56,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%| | 243/293 [04:28<01:01,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%| | 244/293 [04:29<00:54,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%| | 245/293 [04:30<00:54,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%| | 246/293 [04:32<00:59,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%| | 247/293 [04:33<00:54,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%| | 248/293 [04:34<00:52,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%| | 249/293 [04:35<00:50,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%| | 250/293 [04:36<00:45,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%| | 251/293 [04:37<00:41,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%| | 252/293 [04:38<00:40,  1.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%| | 253/293 [04:39<00:40,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%| | 254/293 [04:40<00:44,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%| | 255/293 [04:41<00:39,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%| | 256/293 [04:42<00:37,  1.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%| | 257/293 [04:43<00:37,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%| | 258/293 [04:44<00:36,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%| | 259/293 [04:45<00:36,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%| | 260/293 [04:47<00:40,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%| | 261/293 [04:48<00:36,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%| | 262/293 [04:49<00:38,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%| | 263/293 [04:50<00:33,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%| | 264/293 [04:51<00:30,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%| | 265/293 [04:52<00:28,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%| | 266/293 [04:53<00:28,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%| | 267/293 [04:55<00:36,  1.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|| 268/293 [04:56<00:32,  1.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|| 269/293 [04:57<00:29,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|| 270/293 [04:59<00:31,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|| 271/293 [05:00<00:29,  1.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|| 272/293 [05:01<00:25,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|| 273/293 [05:02<00:23,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|| 274/293 [05:03<00:21,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|| 275/293 [05:04<00:19,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|| 276/293 [05:06<00:19,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|| 277/293 [05:07<00:18,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|| 278/293 [05:08<00:18,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|| 279/293 [05:09<00:17,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|| 280/293 [05:10<00:14,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|| 281/293 [05:12<00:13,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|| 282/293 [05:13<00:14,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|| 283/293 [05:15<00:13,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|| 284/293 [05:16<00:11,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|| 285/293 [05:17<00:09,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|| 286/293 [05:18<00:08,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|| 287/293 [05:19<00:06,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|| 288/293 [05:20<00:06,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|| 289/293 [05:22<00:04,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|| 290/293 [05:23<00:03,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|| 291/293 [05:24<00:02,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 292/293 [05:25<00:01,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 293/293 [05:27<00:00,  1.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/32 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|         | 1/32 [00:00<00:29,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|         | 2/32 [00:01<00:26,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|         | 3/32 [00:03<00:34,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|        | 4/32 [00:04<00:31,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|        | 5/32 [00:05<00:27,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|        | 6/32 [00:06<00:25,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|       | 7/32 [00:07<00:29,  1.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|       | 8/32 [00:08<00:26,  1.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|       | 9/32 [00:10<00:28,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|      | 10/32 [00:11<00:24,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|      | 11/32 [00:11<00:21,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|      | 12/32 [00:12<00:20,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|      | 13/32 [00:14<00:20,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|     | 14/32 [00:15<00:22,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|     | 15/32 [00:16<00:20,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|     | 16/32 [00:17<00:18,  1.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|    | 17/32 [00:18<00:16,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|    | 18/32 [00:19<00:14,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|    | 19/32 [00:20<00:13,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|   | 20/32 [00:22<00:13,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|   | 21/32 [00:23<00:12,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|   | 22/32 [00:24<00:12,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|  | 23/32 [00:25<00:11,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|  | 24/32 [00:27<00:09,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|  | 25/32 [00:28<00:07,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%| | 26/32 [00:29<00:07,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%| | 27/32 [00:30<00:06,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%| | 28/32 [00:31<00:04,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%| | 29/32 [00:34<00:04,  1.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|| 30/32 [00:34<00:02,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|| 31/32 [00:35<00:01,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 32/32 [00:36<00:00,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "\n",
        "train_dataset = generate_qa_embedding_pairs(\n",
        "    llm=OpenAI(model=\"gpt-4o-mini\"),\n",
        "    nodes=train_nodes,\n",
        "    output_path='train_dataset.json'\n",
        ")\n",
        "\n",
        "val_dataset = generate_qa_embedding_pairs(\n",
        "    llm=OpenAI(model=\"gpt-4o-mini\"),\n",
        "    nodes=val_nodes,\n",
        "    output_path='val_dataset.json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2964972",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "743f163c-25df-4c18-9abe-05052b034d70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "743f163c-25df-4c18-9abe-05052b034d70",
        "outputId": "f6c35b76-173a-406e-978b-1fb245f82cdc"
      },
      "outputs": [],
      "source": [
        "# [Optional] Load\n",
        "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n",
        "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cfdf868",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f10e500",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "62368cb8-a303-48b1-8429-5e3655abcc3b",
      "metadata": {
        "id": "62368cb8-a303-48b1-8429-5e3655abcc3b"
      },
      "source": [
        "## Run Embedding Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d08066-5f00-48f1-b12a-e80bc193d4c0",
      "metadata": {
        "id": "c1d08066-5f00-48f1-b12a-e80bc193d4c0"
      },
      "outputs": [],
      "source": [
        "from llama_index.finetuning import SentenceTransformersFinetuneEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26625ab5-ddc9-4dbd-9936-39b69c6a7cdc",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6efae9c64cdc4a92a248cf1619349958",
            "82a58350abe74a59b39686504e56ddb7",
            "c938a8515da340fa8567502eb4ab1379",
            "cec7526cf6d74ab5a90b5a2adecb8dcf",
            "e8177356c92541939bdc0d7f51a88dd2",
            "1155eb2d55b446639814729da89f2a8f",
            "8db118818c7e4dacb623944d8888e0a2",
            "2c7498da6664460ab67ac5fc72fcd565",
            "dfa0168357b74c7f900e49c4cb38b4eb",
            "39c5ef016f2f4d73bd9d78e081b46f47",
            "2d40419c72754123b95d7a4f3430cac3",
            "aba92340280a4601a19f4a8707c45fba",
            "e03c93e272574b46a7bb8ca5e389b354"
          ]
        },
        "id": "26625ab5-ddc9-4dbd-9936-39b69c6a7cdc",
        "outputId": "11488020-53a0-4252-8651-f66a6dc09f77"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6efae9c64cdc4a92a248cf1619349958",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82a58350abe74a59b39686504e56ddb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c938a8515da340fa8567502eb4ab1379",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/90.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cec7526cf6d74ab5a90b5a2adecb8dcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8177356c92541939bdc0d7f51a88dd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1155eb2d55b446639814729da89f2a8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8db118818c7e4dacb623944d8888e0a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c7498da6664460ab67ac5fc72fcd565",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfa0168357b74c7f900e49c4cb38b4eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39c5ef016f2f4d73bd9d78e081b46f47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d40419c72754123b95d7a4f3430cac3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aba92340280a4601a19f4a8707c45fba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e03c93e272574b46a7bb8ca5e389b354",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_model = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
        "finetuned_model = 'policy_gte_large_5'\n",
        "\n",
        "finetune_engine = SentenceTransformersFinetuneEngine(\n",
        "    train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    model_id=base_model,\n",
        "    model_output_path=finetuned_model,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ad99e6-dd9d-485a-86e9-1845cf51802b",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "78dab7a09dd640619d80e986baf37249",
            "b025b81ebe21403498679bf916626ff9",
            "e737aae9a5f4459c97df630e63b9c487"
          ]
        },
        "id": "28ad99e6-dd9d-485a-86e9-1845cf51802b",
        "outputId": "1f9a256d-db4f-43f4-90e6-1569e25c7b99"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78dab7a09dd640619d80e986baf37249",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b025b81ebe21403498679bf916626ff9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/69 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e737aae9a5f4459c97df630e63b9c487",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/69 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "finetune_engine.finetune()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467a2ba2-e7e6-4025-8887-cac6e7ecb493",
      "metadata": {
        "id": "467a2ba2-e7e6-4025-8887-cac6e7ecb493"
      },
      "outputs": [],
      "source": [
        "embed_model = finetune_engine.get_finetuned_model()\n",
        "embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d16ec01-c29d-4742-aa3c-5ded6ae7c5a7",
      "metadata": {
        "id": "5d16ec01-c29d-4742-aa3c-5ded6ae7c5a7",
        "outputId": "c1f365b4-cc69-432e-8ea9-7256abda76c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HuggingFaceEmbedding(model_name='test_model', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x2cc3d5cd0>, tokenizer_name='test_model', max_length=512, pooling=<Pooling.CLS: 'cls'>, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b8c07a",
      "metadata": {},
      "outputs": [],
      "source": [
        "!optimum-cli export onnx --model policy_gte_large_5/ policy_gte_large_5/onnx/ --task feature-extraction --trust-remote-code --framework pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c933d0cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "finetune_engine.model.push_to_hub(finetuned_model, local_model_path=finetuned_model, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "828dd6fe-9a8a-419b-8663-56d81ce73774",
      "metadata": {
        "id": "828dd6fe-9a8a-419b-8663-56d81ce73774"
      },
      "source": [
        "## Evaluate Finetuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a66b83-4cbb-4374-a632-0f1bb2b785ab",
      "metadata": {
        "id": "f4a66b83-4cbb-4374-a632-0f1bb2b785ab"
      },
      "source": [
        "In this section, we evaluate 3 different embedding models:\n",
        "1. proprietary OpenAI embedding,\n",
        "2. open source `BAAI/bge-small-en`, and\n",
        "3. our finetuned embedding model.\n",
        "\n",
        "We consider 2 evaluation approaches:\n",
        "1. a simple custom **hit rate** metric\n",
        "2. using `InformationRetrievalEvaluator` from sentence_transformers\n",
        "\n",
        "We show that finetuning on synthetic (LLM-generated) dataset significantly improve upon an opensource embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d5176f-1f21-4bcb-adf5-da1c4cccb8d3",
      "metadata": {
        "id": "57d5176f-1f21-4bcb-adf5-da1c4cccb8d3"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.schema import TextNode\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda4c2b8-1ad8-420c-83d2-b88e0519895d",
      "metadata": {
        "id": "dda4c2b8-1ad8-420c-83d2-b88e0519895d"
      },
      "source": [
        "### Define eval function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398c24d3-3d72-4ce8-94a4-2da9c1b2605c",
      "metadata": {
        "id": "398c24d3-3d72-4ce8-94a4-2da9c1b2605c"
      },
      "source": [
        "**Option 1**: We use a simple **hit rate** metric for evaluation:\n",
        "* for each (query, relevant_doc) pair,\n",
        "* we retrieve top-k documents with the query,  and\n",
        "* it's a **hit** if the results contain the relevant_doc.\n",
        "\n",
        "This approach is very simple and intuitive, and we can apply it to both the proprietary OpenAI embedding as well as our open source and fine-tuned embedding models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89401d3-a157-4f96-86d4-212e631a54bc",
      "metadata": {
        "id": "b89401d3-a157-4f96-86d4-212e631a54bc"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=5,\n",
        "    verbose=False,\n",
        "):\n",
        "    corpus = dataset.corpus\n",
        "    queries = dataset.queries\n",
        "    relevant_docs = dataset.relevant_docs\n",
        "\n",
        "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
        "    index = VectorStoreIndex(\n",
        "        nodes, embed_model=embed_model, show_progress=True\n",
        "    )\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "\n",
        "    eval_results = []\n",
        "    for query_id, query in tqdm(queries.items()):\n",
        "        retrieved_nodes = retriever.retrieve(query)\n",
        "        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n",
        "        expected_id = relevant_docs[query_id][0]\n",
        "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
        "\n",
        "        eval_result = {\n",
        "            \"is_hit\": is_hit,\n",
        "            \"retrieved\": retrieved_ids,\n",
        "            \"expected\": expected_id,\n",
        "            \"query\": query_id,\n",
        "        }\n",
        "        eval_results.append(eval_result)\n",
        "    return eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eb16251-bb45-4de0-b65a-e15aa76e0f1e",
      "metadata": {
        "id": "7eb16251-bb45-4de0-b65a-e15aa76e0f1e"
      },
      "source": [
        "**Option 2**: We use the `InformationRetrievalEvaluator` from sentence_transformers.\n",
        "\n",
        "This provides a more comprehensive suite of metrics, but we can only run it against the sentencetransformers compatible models (open source and our finetuned model, *not* the OpenAI embedding model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e89702-ea35-4c22-99c7-f89a5428ef95",
      "metadata": {
        "id": "88e89702-ea35-4c22-99c7-f89a5428ef95"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def evaluate_st(\n",
        "    dataset,\n",
        "    model_id,\n",
        "    name,\n",
        "):\n",
        "    corpus = dataset.corpus\n",
        "    queries = dataset.queries\n",
        "    relevant_docs = dataset.relevant_docs\n",
        "\n",
        "    evaluator = InformationRetrievalEvaluator(\n",
        "        queries, corpus, relevant_docs, name=name\n",
        "    )\n",
        "    model = SentenceTransformer(model_id)\n",
        "    output_path = \"results/\"\n",
        "    Path(output_path).mkdir(exist_ok=True, parents=True)\n",
        "    return evaluator(model, output_path=output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2d33dd-c39f-4c05-8adc-65db12163c88",
      "metadata": {
        "id": "af2d33dd-c39f-4c05-8adc-65db12163c88"
      },
      "source": [
        "### Run Evals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c630aa25-2395-4a8b-83cf-2885fbc862f4",
      "metadata": {
        "id": "c630aa25-2395-4a8b-83cf-2885fbc862f4"
      },
      "source": [
        "#### OpenAI\n",
        "\n",
        "Note: this might take a few minutes to run since we have to embed the corpus and queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a0784f-415e-4d3a-8c88-757b28b9e5df",
      "metadata": {
        "id": "61a0784f-415e-4d3a-8c88-757b28b9e5df"
      },
      "outputs": [],
      "source": [
        "ada = OpenAIEmbedding()\n",
        "ada_val_results = evaluate(val_dataset, ada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc73212-fc53-48c1-b347-f5ee3a29ae82",
      "metadata": {
        "id": "ccc73212-fc53-48c1-b347-f5ee3a29ae82"
      },
      "outputs": [],
      "source": [
        "df_ada = pd.DataFrame(ada_val_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25eb61bb-c287-40fe-b3c7-bbfc2d2b1b94",
      "metadata": {
        "id": "25eb61bb-c287-40fe-b3c7-bbfc2d2b1b94",
        "outputId": "44bbd12b-f339-4102-f53e-23fb6bd25392"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8779904306220095"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hit_rate_ada = df_ada[\"is_hit\"].mean()\n",
        "hit_rate_ada"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1bd6c62-65a8-4f72-a67c-d0d62c92d7d1",
      "metadata": {
        "id": "a1bd6c62-65a8-4f72-a67c-d0d62c92d7d1"
      },
      "source": [
        "### BAAI/bge-small-en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24454aeb-9e3e-4954-ab70-647102ed7f82",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6e9c5f0555f641caa3a5a5d11cb87583",
            "1fe9a221f8984c818727771d12dfef71",
            "619c9cae8bf24987a4d3453aa69d24b9",
            "082cfe7c9f3646948886c90f0e1f4258",
            "3ff8d7a739fc425abf24076c47c0ab29",
            "0a5344851cb14ed8a5f788cbd74a90d8",
            "eaa8bdab99244058b1df3eae12a79b20",
            "e21b1a35d6c54644be124c357852fedf",
            "927efec699ea4c929da7214eb51fc64c",
            "1c8a00d15090422181a9749e0638e883",
            "3845bc276c88482ba0e2f2fbe317dd78",
            "7ceca7b6507e42b1b3da10711b37b7ab",
            "21170e7cf0f9485a9095807a6225aa12",
            "3712232b7e064486879945c4d4ac5535",
            "ba1f47ec020447c59d008493b31e0a57"
          ]
        },
        "id": "24454aeb-9e3e-4954-ab70-647102ed7f82",
        "outputId": "bb1f9e84-0be2-4cd8-ec8b-407729613185"
      },
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.011851787567138672,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()ab102/.gitattributes",
              "rate": null,
              "total": 1519,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e9c5f0555f641caa3a5a5d11cb87583",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()ab102/.gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.009984016418457031,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()_Pooling/config.json",
              "rate": null,
              "total": 190,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fe9a221f8984c818727771d12dfef71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.006695985794067383,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()2d2d7ab102/README.md",
              "rate": null,
              "total": 78855,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "619c9cae8bf24987a4d3453aa69d24b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()2d2d7ab102/README.md:   0%|          | 0.00/78.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.008507966995239258,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()2d7ab102/config.json",
              "rate": null,
              "total": 684,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "082cfe7c9f3646948886c90f0e1f4258",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()2d7ab102/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.011520147323608398,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()ce_transformers.json",
              "rate": null,
              "total": 124,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ff8d7a739fc425abf24076c47c0ab29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()ce_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.010421991348266602,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading model.safetensors",
              "rate": null,
              "total": 133466304,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a5344851cb14ed8a5f788cbd74a90d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.010251045227050781,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading pytorch_model.bin",
              "rate": null,
              "total": 133508397,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaa8bdab99244058b1df3eae12a79b20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.012272834777832031,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()nce_bert_config.json",
              "rate": null,
              "total": 52,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e21b1a35d6c54644be124c357852fedf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()nce_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0071942806243896484,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()cial_tokens_map.json",
              "rate": null,
              "total": 125,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "927efec699ea4c929da7214eb51fc64c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.010439872741699219,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()ab102/tokenizer.json",
              "rate": null,
              "total": 711396,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c8a00d15090422181a9749e0638e883",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()ab102/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005940914154052734,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()okenizer_config.json",
              "rate": null,
              "total": 366,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3845bc276c88482ba0e2f2fbe317dd78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()okenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.00838780403137207,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()2d2d7ab102/vocab.txt",
              "rate": null,
              "total": 231508,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ceca7b6507e42b1b3da10711b37b7ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()2d2d7ab102/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.007328987121582031,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Downloading ()d7ab102/modules.json",
              "rate": null,
              "total": 229,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21170e7cf0f9485a9095807a6225aa12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()d7ab102/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.001703023910522461,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "Generating embeddings",
              "rate": null,
              "total": 418,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3712232b7e064486879945c4d4ac5535",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/418 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0017049312591552734,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": 28,
              "postfix": null,
              "prefix": "",
              "rate": null,
              "total": 836,
              "unit": "it",
              "unit_divisor": 1000,
              "unit_scale": false
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba1f47ec020447c59d008493b31e0a57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/836 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bge = \"local:BAAI/bge-small-en\"\n",
        "bge_val_results = evaluate(val_dataset, bge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da27e48-1c90-4994-aac4-96b5b1638647",
      "metadata": {
        "id": "2da27e48-1c90-4994-aac4-96b5b1638647"
      },
      "outputs": [],
      "source": [
        "df_bge = pd.DataFrame(bge_val_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddc4fe0-b240-4c15-9b2d-a4c79f9aaac2",
      "metadata": {
        "id": "3ddc4fe0-b240-4c15-9b2d-a4c79f9aaac2",
        "outputId": "cdbfd928-cc13-48aa-8df0-d6d9252b0620"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7930622009569378"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hit_rate_bge = df_bge[\"is_hit\"].mean()\n",
        "hit_rate_bge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c16df14-6564-41ec-8816-348675bb0fd4",
      "metadata": {
        "id": "2c16df14-6564-41ec-8816-348675bb0fd4",
        "outputId": "e0e13cfc-6e66-4a5d-effb-96284f710b99"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'results/Information-Retrieval_evaluation_bge_results.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_st\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-small-en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[49], line 15\u001b[0m, in \u001b[0;36mevaluate_st\u001b[0;34m(dataset, model_id, name)\u001b[0m\n\u001b[1;32m     13\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m InformationRetrievalEvaluator(queries, corpus, relevant_docs, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_id)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/sentence_transformers/evaluation/InformationRetrievalEvaluator.py:104\u001b[0m, in \u001b[0;36mInformationRetrievalEvaluator.__call__\u001b[0;34m(self, model, output_path, epoch, steps, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcsv_file)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(csv_path):\n\u001b[0;32m--> 104\u001b[0m     fOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     fOut\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcsv_headers))\n\u001b[1;32m    106\u001b[0m     fOut\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/Information-Retrieval_evaluation_bge_results.csv'"
          ]
        }
      ],
      "source": [
        "evaluate_st(val_dataset, \"BAAI/bge-small-en\", name=\"bge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd87550-f547-4b8b-b21a-f72b355e2cd7",
      "metadata": {
        "id": "1fd87550-f547-4b8b-b21a-f72b355e2cd7"
      },
      "source": [
        "### Finetuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402dd440-1934-4778-8ff5-28e15cf1f2d3",
      "metadata": {
        "id": "402dd440-1934-4778-8ff5-28e15cf1f2d3"
      },
      "outputs": [],
      "source": [
        "finetuned = \"local:test_model\"\n",
        "val_results_finetuned = evaluate(val_dataset, finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd24643-17cb-4773-a535-77f3f8fa2d48",
      "metadata": {
        "id": "ffd24643-17cb-4773-a535-77f3f8fa2d48"
      },
      "outputs": [],
      "source": [
        "df_finetuned = pd.DataFrame(val_results_finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec1dccd1-bbd4-427f-a520-b1011643d83b",
      "metadata": {
        "id": "ec1dccd1-bbd4-427f-a520-b1011643d83b"
      },
      "outputs": [],
      "source": [
        "hit_rate_finetuned = df_finetuned[\"is_hit\"].mean()\n",
        "hit_rate_finetuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8dd38e-f13d-43e1-9802-cc94b854526b",
      "metadata": {
        "id": "9d8dd38e-f13d-43e1-9802-cc94b854526b"
      },
      "outputs": [],
      "source": [
        "evaluate_st(val_dataset, \"test_model\", name=\"finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc290bc-5cc3-4ee4-b8ab-e68371441643",
      "metadata": {
        "id": "fbc290bc-5cc3-4ee4-b8ab-e68371441643"
      },
      "source": [
        "### Summary of Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f906a11-6a95-4f10-9069-140bf5a56246",
      "metadata": {
        "id": "6f906a11-6a95-4f10-9069-140bf5a56246"
      },
      "source": [
        "#### Hit rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "705fbe3c-2843-4bab-bb5c-16027fc5564b",
      "metadata": {
        "id": "705fbe3c-2843-4bab-bb5c-16027fc5564b"
      },
      "outputs": [],
      "source": [
        "df_ada[\"model\"] = \"ada\"\n",
        "df_bge[\"model\"] = \"bge\"\n",
        "df_finetuned[\"model\"] = \"fine_tuned\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebc363c-cd07-4dab-916e-1618d16d1254",
      "metadata": {
        "id": "bebc363c-cd07-4dab-916e-1618d16d1254"
      },
      "source": [
        "We can see that fine-tuning our small open-source embedding model drastically improve its retrieval quality (even approaching the quality of the proprietary OpenAI embedding)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f38b4b-1b40-42da-a054-ea9593d3e602",
      "metadata": {
        "id": "57f38b4b-1b40-42da-a054-ea9593d3e602"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat([df_ada, df_bge, df_finetuned])\n",
        "df_all.groupby(\"model\").mean(\"is_hit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08094d07-2c0a-44ca-ad2f-8d8bf1387ed9",
      "metadata": {
        "id": "08094d07-2c0a-44ca-ad2f-8d8bf1387ed9"
      },
      "source": [
        "#### InformationRetrievalEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d0444e-a824-42d6-9ddb-4da7179902bc",
      "metadata": {
        "id": "27d0444e-a824-42d6-9ddb-4da7179902bc"
      },
      "outputs": [],
      "source": [
        "df_st_bge = pd.read_csv(\n",
        "    \"results/Information-Retrieval_evaluation_bge_results.csv\"\n",
        ")\n",
        "df_st_finetuned = pd.read_csv(\n",
        "    \"results/Information-Retrieval_evaluation_finetuned_results.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0903ed3-df05-4d98-8b0a-6f352c681735",
      "metadata": {
        "id": "c0903ed3-df05-4d98-8b0a-6f352c681735"
      },
      "source": [
        "We can see that embedding finetuning improves metrics consistently across the suite of eval metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ec1c46-5aa0-4f8a-a0c5-2553e08cceb1",
      "metadata": {
        "id": "81ec1c46-5aa0-4f8a-a0c5-2553e08cceb1"
      },
      "outputs": [],
      "source": [
        "df_st_bge[\"model\"] = \"bge\"\n",
        "df_st_finetuned[\"model\"] = \"fine_tuned\"\n",
        "df_st_all = pd.concat([df_st_bge, df_st_finetuned])\n",
        "df_st_all = df_st_all.set_index(\"model\")\n",
        "df_st_all"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
